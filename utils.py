import torch
import math
import random
from torch import nn
import torch.nn.functional as F


def test_answer_mmlu_(pred_str, ans):
    pattern = 'the answer is ('
    pred = pred_str.lower().split(pattern)

    if (len(pred) > 1):
        # print(pred)
        pred = pred[1][0]
        gold = ans.lower()
        # print('debug 1, pred %s, gold %s' % (pred, gold))
        return pred == gold
    else:
        pred = 'C'
        # print(ans_str)
        gold = ans.lower()
        # print('debug 2, pred %s, gold %s' % (pred, gold))
        return pred == gold


# extract answer in pred_str and compare with ans_str
def test_answer_mmlu_claude_instant(pred_str, ans_str):
    pattern = 'the answer is '
    pred = pred_str.lower().split(pattern)
    if len(pred) == 1:
        return False
    else:
        return pred[1][0] == ans_str.lower()


def test_answer_mmlu_claude(pred_str, ans_str):
    pattern = 'the answer is '
    pred = pred_str.lower().split(pattern)

    if (len(pred) > 1):
        # print(pred)
        pred = pred[1]
        for p in pred:
            if (p.isalpha()): break
        pred = p
        print(ans_str)
        gold = ans_str.lower()
        print('debug 1, pred %s, gold %s' % (pred, gold))
        return pred == gold
    else:
        pred = 'c'
        # print(ans_str)
        gold = ans_str.lower()
        # print('debug 2, pred %s, gold %s' % (pred, gold))
        return pred == gold


def test_answer_mmlu(pred_str, ans_str):
    pattern = 'the answer is ('
    pred = pred_str.lower().split(pattern)

    if (len(pred) > 1):
        # print(pred)
        pred = pred[1][0]
        gold = ans_str.split('A:\n')[1][0].lower()
        # print('debug 1, pred %s, gold %s' % (pred, gold))
        return pred == gold
    else:
        pred = 'C'
        # print(ans_str)
        gold = ans_str.split('A:\n')[1][0].lower()
        # print('debug 2, pred %s, gold %s' % (pred, gold))
        return pred == gold


def parse_pred_ans(filename):
    with open(filename) as fd:
        lines = fd.readlines()
    am, a = None, None
    num_q, acc = 0, 0
    current_mode = 'none'
    questions = []
    ans_pred = []
    ans_gold = []
    for l in lines:
        if (l.startswith('Q: ')):
            if (am is not None and a is not None):
                questions.append(q)
                ans_pred.append(am)
                ans_gold.append(a)
                # print(am)
                # print(a)
                if (test_answer_mmlu(am, a)):
                    acc += 1
            current_mode = 'q'
            q = l
            num_q += 1
        elif (l.startswith('A_model:')):
            current_mode = 'am'
            am = l
        elif (l.startswith('A:') and not l.startswith("A: Let's think step by step")):
            current_mode = 'a'
            a = l
        else:
            if (current_mode == 'q'):
                q += l
            elif (current_mode == 'am'):
                am += l
            elif (current_mode == 'a'):
                a += l
            else:
                raise ValueError(current_mode)

    questions.append(q)
    ans_pred.append(am)
    ans_gold.append(a)
    # print(am)
    # print(a)
    if (test_answer_mmlu(am, a)):
        acc += 1
    print('num_q %d correct %d ratio %.4f' % (num_q, acc, float(acc / num_q)))
    return questions, ans_pred, ans_gold


def test_finished(ans_model):
    if ('answer is' in ans_model):
        return True
    else:
        return False


def extract_ans(ans_model):
    ans_model = ans_model.split('\n')
    ans = []
    residual = []
    for li, al in enumerate(ans_model):
        ans.append(al)
        if ('answer is' in al):
            break
    residual = list(ans_model[li + 1:])
    ans = '\n'.join(ans)
    residual = '\n'.join(residual)
    return ans, residual


def quantize_weight(weight: torch.Tensor, clip_val, num_bits: int, fake_quant=True):
    """
    :param     weight: Weight need to be quantized
    :param   clip_val: None or (min, max) tuple
    :param   num_bits: quantization bit, recommend 2, 4, 8, 16
    :param fake_quant: true if return dequantized fp32 weight else return real quantized int number;
                       only support int8 and int16
    :return: quantized weight
    """

    if clip_val is None:
        # Automatically find the clip values
        # Assume the weight is Gaussian distribution
        # For small bits, discard more extreme values
        mean, std = weight.mean(), weight.std()
        clip_val = (mean - 2 * std, mean + 2 * std) if num_bits < 8 else (mean - 4 * std, mean + 4 * std)

    weight = torch.where(weight > clip_val[0], weight, clip_val[0])
    weight = torch.where(weight < clip_val[1], weight, clip_val[1])

    # DEBUG
    truncate_proportion = torch.where(weight == clip_val[0], 1.0, 0.0).mean()
    truncate_proportion += torch.where(weight == clip_val[1], 1.0, 0.0).mean()
    truncate_proportion = 100 * truncate_proportion.mean()
    print(f"Min: {clip_val[0]} | Max: {clip_val[1]} | Proportion: {truncate_proportion:.2f}")

    alpha = (weight.max() - weight.min()).detach()
    beta = weight.min().detach()

    weight_normalized = (weight - beta) / (alpha + 1e-8)  # normalize the weight into 0~1
    s = 2 ** num_bits - 1
    quant_weight = torch.round(weight_normalized * s).div(s)  # quantize the weight
    quant_weight[weight == 0] = 0
    if fake_quant:
        fake_quant_weight = quant_weight * (alpha + 1e-8) + beta  # dequantize the weight for training convenience
        return fake_quant_weight
    else:
        if num_bits == 8:
            real_quant_weight = quant_weight.type(torch.int8)
        elif num_bits == 16:
            real_quant_weight = quant_weight.type(torch.int16)
        else:
            raise ValueError(f"int{num_bits} not supported. Only support int8 and int16.")

        return real_quant_weight, alpha, beta


def low_rank_decomposition(weight, reduced_rank=32):
    """
    :param          weight: The matrix to decompose, of shape (H, W)
    :param    reduced_rank: the final rank
    :return:
    """

    """parameter_ratio = rank * (H + W) / (H * W)"""
    """rank_ratio = """
    matrix_dimension = len(weight.size())
    assert matrix_dimension == 2, "Only Support 2D matrix"
    H, W = weight.size()

    # Use SVD to decompose a matrix, default full_matrices is False to save parameters
    U, S, Vh = torch.linalg.svd(weight, full_matrices=False)
    rank = torch.count_nonzero(S)
    is_full_rank = rank == min(H, W)

    L = U @ (torch.sqrt(torch.diag(S)[:, 0:reduced_rank]))
    R = torch.sqrt(torch.diag(S)[0:reduced_rank, :]) @ Vh

    print(f"W: ({H},{W}) | Rank: {rank} | U:{U.shape} | S:{S.shape} | Vh:{Vh.shape}")
    print(f"Reduced Rank: {reduced_rank} | Num Parameters: {(H + W) * reduced_rank}")
    print(f"L: {L.shape} | R: {R.shape}")

    return {"L": L, "R": R, "U": U, "S": S, "Vh": Vh, 'reduced_rank': reduced_rank}


class LinearQuantAct(nn.Linear):
    def quantize_activation(self, x):
        x_ = x.clone()
        mean, std = x.mean(), x.std()
        min_val, max_val = mean - 1 * std, mean + 1 * std
        x_ = torch.where(x_ > min_val, x_, min_val)
        x_ = torch.where(x_ < max_val, x_, max_val)

        alpha = max_val - min_val
        beta = min_val

        x_ = (x_ - beta) / (alpha + 1e-8)  # normalize the activation into 0~1
        s = 2 ** self.num_bits - 1
        x_ = torch.round(x_ * s).div(s)  # quantize the activation
        x_int = x_ * (alpha + 1e-8) + beta  # dequantize the weight for training convenience
        x_fp = x - x_int

        return x_int, x_fp

    def forward(self, x):
        x_int, x_fp = self.quantize_activation(x)
        return F.linear(x_int, self.weight, self.bias)



class LinearQuantLoRA(nn.Module):
    def __init__(self, in_feature, out_feature, reduced_rank, num_bits, has_bias=True, quant_act=False):
        super().__init__()
        self.in_feature = in_feature
        self.out_feature = out_feature
        self.reduced_rank = reduced_rank
        self.num_bits = num_bits
        self.has_bias = has_bias
        self.quant_act = quant_act
        if self.quant_act:
            print("Activatino Quantization Enabled")

        self.quant = nn.Linear(in_feature, out_feature, bias=False)
        self.right = nn.Linear(in_feature, reduced_rank, bias=False)
        self.left = nn.Linear(reduced_rank, out_feature, bias=False)
        if self.has_bias:
            self.bias = nn.Parameter(torch.zeros(out_feature, requires_grad=True))

    def quantize_activation(self, x):
        x_ = x.clone()
        mean, std = x.mean(), x.std()
        min_val, max_val = mean - 1 * std, mean + 1 * std
        x_ = torch.where(x_ > min_val, x_, min_val)
        x_ = torch.where(x_ < max_val, x_, max_val)

        alpha = max_val - min_val
        beta = min_val

        x_ = (x_ - beta) / (alpha + 1e-8)  # normalize the activation into 0~1
        s = 2 ** self.num_bits - 1
        x_ = torch.round(x_ * s).div(s)  # quantize the activation
        x_int = x_ * (alpha + 1e-8) + beta  # dequantize the weight for training convenience
        x_fp = x - x_int

        return x_int, x_fp

    def forward(self, x):
        if self.quant_act:
            """Y = (H+LR)(X_int + X_fp) ~= HX*X_int + LR*X_fp"""
            x_int, x_fp = self.quantize_activation(x)
            # LRX = self.left(self.right(x_fp))
            HX = self.quant(x_int)
        else:
            """Y = (H + LR)X = HX + LRX"""
            # LRX = self.left(self.right(x))
            HX = self.quant(x)

        #Y = HX + LRX + self.bias if self.has_bias else HX + LRX
        Y = HX + self.bias if self.has_bias else HX
        return Y

    def initialize_weight(self, quant_weight, left_weight, right_weight, bias=None):
        self.quant.weight = nn.Parameter(quant_weight, requires_grad=False)  # Freeze the backbone
        self.left.weight = nn.Parameter(left_weight, requires_grad=True)
        self.right.weight = nn.Parameter(right_weight, requires_grad=True)
        if self.has_bias:
            self.bias = nn.Parameter(bias, requires_grad=True)


class Linear(nn.Linear):
    def forward(self, input: torch.Tensor) -> torch.Tensor:
        ret = input @ self.weight.T
        if self.bias is None:
            return ret
        else:
            return ret + self.bias


def substitute_layer_weights_quant_svd(module,
                                       allow_name=None,
                                       block_name=None,
                                       reduced_rank=32,
                                       svd_init=True,
                                       num_bits=4,
                                       act_quant=False):
    """
    :param         num_bit: integer bit, 8, 4, 2 for example
    :param        svd_init: operate SVD initialization, otherwise LoRA initialization
    :param          module: an nn.Module class
    :param      block_name: do not continue to iterate when the module's name is in the block_name
    :param      allow_name: replace the module if its name is in the allow_name
    :param    reduced_rank: reduced rank
    :return: None
    """

    # Default allow name and block name lists
    if allow_name is None:
        allow_name = ['query', 'key', 'value', 'dense', 'attention']
    if block_name is None:
        block_name = ['pooler', 'classifier', 'LayerNorm', 'embeddings']

    for attr_str in dir(module):
        target_attr = getattr(module, attr_str)
        if (isinstance(target_attr, nn.Linear) or isinstance(target_attr, Linear)) and any(an in attr_str for an in allow_name):
            print("====================================================")
            print(attr_str, target_attr)

            if svd_init:
                # Uniformly quantize the weight
                weight = target_attr.weight.data.to('cuda')
                quant_weight = quantize_weight(weight,
                                               clip_val=None,
                                               num_bits=num_bits,
                                               fake_quant=True)
                residual_1 = weight - quant_weight

                # Decompose the residual_1 by SVD
                output = low_rank_decomposition(residual_1, reduced_rank=reduced_rank)
                L, R, reduced_rank = output['L'], output['R'], output['reduced_rank']
                # S = residual_1 - torch.mm(L, R)

            else:
                H, W = target_attr.weight.shape
                L = torch.zeros(H, reduced_rank, requires_grad=True)
                R = torch.randn((reduced_rank, W), requires_grad=True)
                quant_weight = quantize_weight(target_attr.weight,
                                               clip_val=None,
                                               num_bits=num_bits,
                                               fake_quant=True)

            # Create a nn.Module and assign decomposed weights to the parameters
            linear_loras = LinearQuantLoRA(target_attr.in_features, target_attr.out_features, reduced_rank,
                                           num_bits=num_bits,
                                           has_bias=True if target_attr.bias is not None else False,
                                           quant_act=act_quant)

            linear_loras.initialize_weight(quant_weight, L, R, target_attr.bias)

            setattr(module, attr_str, linear_loras)

    for name, immediate_child_module in module.named_children():
        # do not continue to iterate when the module's name is in the block_name
        if not any(name in bn for bn in block_name):
            substitute_layer_weights_quant_svd(immediate_child_module, allow_name, block_name, reduced_rank,
                                               svd_init, num_bits, act_quant)



def substitute_layer_weights_quant_act(module,
                                       allow_name=None,
                                       block_name=None,
                                       num_bits=4,):
    """
    :param         num_bit: integer bit, 8, 4, 2 for example
    :param        svd_init: operate SVD initialization, otherwise LoRA initialization
    :param          module: an nn.Module class
    :param      block_name: do not continue to iterate when the module's name is in the block_name
    :param      allow_name: replace the module if its name is in the allow_name
    :param    reduced_rank: reduced rank
    :return: None
    """

    # Default allow name and block name lists
    if allow_name is None:
        allow_name = ['query', 'key', 'value', 'dense', 'attention']
    if block_name is None:
        block_name = ['pooler', 'classifier', 'LayerNorm', 'embeddings']

    for attr_str in dir(module):
        target_attr = getattr(module, attr_str)
        if (isinstance(target_attr, nn.Linear) or isinstance(target_attr, Linear)) and any(an in attr_str for an in allow_name):
            print("====================================================")
            print(attr_str, target_attr)

            quant_weight = quantize_weight(target_attr.weight,
                                           clip_val=None,
                                           num_bits=num_bits,
                                           fake_quant=True)

            # Create a nn.Module and assign decomposed weights to the parameters
            linear_loras = LinearQuantAct(target_attr.in_features, target_attr.out_features,
                                          bias=True if target_attr.bias is not None else False)

            linear_loras.weight = nn.Parameter(quant_weight)
            if target_attr.bias is not None:
                linear_loras.bias = nn.Parameter(target_attr.bias)

            setattr(module, attr_str, linear_loras)

    for name, immediate_child_module in module.named_children():
        # do not continue to iterate when the module's name is in the block_name
        if not any(name in bn for bn in block_name):
            substitute_layer_weights_quant_act(immediate_child_module, allow_name, block_name, num_bits)


if __name__ == '__main__':
    x = torch.randn(1024, 1024)
    a = x.clone()
    num_bits = 8

    mean, std = x.mean(), x.std()
    min_val, max_val = mean - 5 * std, mean + 5 * std
    x = torch.where(x > min_val, x, min_val)
    x = torch.where(x < max_val, x, max_val)

    alpha = max_val - min_val
    beta = min_val

    x = (x - beta) / (alpha + 1e-8)  # normalize the activation into 0~1
    s = 2 ** num_bits - 1
    x = torch.round(x * s).div(s)
    #x = torch.round(x * s).div(s)
    print(x)# quantize the activation
    #
    b = x * (alpha + 1e-8) + beta  # dequantize the weight for training convenience

    error_b = (a - b).pow(2).mean().sqrt().item()
    print(error_b)
